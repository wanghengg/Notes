# 进程和线程的区别

* 进程是分配资源的基本单位，线程是CPU调度的基本单位。
* 进程有独立的地址空间，线程共享本进程的地址空间，从而能访问相同的数据。但是线程也有自己独立的程序计数器PC，每个线程也有自己的一组用于计算的寄存器。
* 线程的切换和进程的切换类似，当从一个线程切换到另一个线程时，会发生上下文切换。对于 进程，上下文切换时将状态保存到进程控制块（PCB），线程切换时将状态保存到线程控制块（TCB）。与进程相比，线程上下文切换时，地址空间保持不变，所以不需要切换当前使用的页表。同时由于线程共享数据，因此线程的切换消耗的资源更少，效率更高。
* 线程和进程之间的另一个主要区别在于栈，传统的进程地址空间模型（或者叫做单线程进程）中，只有一个栈。但是在多线程进程中，每个线程独立运行，每个线程都可以调用各种例程来完成工作，所以每个线程都有一个栈。
* 一个进程崩溃之后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃会让本进程的所有线程都崩溃。
* 执行时，每个独立的进程都有一个程序运行的入口，但是线程不能独立执行，必须依存于进程。

## 进程创建函数

```c++
#include <stdio.h>
#include <pthread.h>
#include <assert.h>

void *mythread(void* arg) {
    printf("%s\n", (char*)arg);
    return NULL;
}
int main() {
    pthread_t p1, p2;
    // param1:线程地址，param2:NULL(默认),para3:要调用的函数，para4:调用函数的参数
    int rc;
    rc = pthread_create(&p1, NULL, mythread, "A");	assert(rc == 0);
    rc = pthread_create(&p2, NULL, mythread, "B");	assert(rc == 0);
    // join wait for the threads to finish
    rc = pthread_join(p1, NULL); assert(rc == 0);
    rc = pthread_join(p2, NULL); assert(rc == 0);
    printf("main: end\n");
    return 0;
}
```

# [Linux IO模式及select、poll、epoll](https://www.cnblogs.com/natian-ws/p/10785649.html)

## 概念说明

1. 内核态（内核空间）和用户态（用户空间）的区别和联系？

   用户空间是用户进程所在的内存区域，内核空间是操作系统所在的内存区域。

   用了保证内核的安全，处于用户态的程序只能访问用户空间，而处于内核态的程序可以访问用户空间和内核空间。

2. 文件描述符fd

   `linux`将所有设备都当做文件来处理，文件描述符来标识每个文件对象。

   当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。

3. 缓存IO

   `Linux`的缓存IO机制中，操作系统会将IO的数据缓存在操作系统内核的缓冲区汇中，数据会先被拷贝到系统内核的缓冲区，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

### IO模式

对于一次IO访问(以read为例)，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，一个read操作发生时，会经历两个阶段：

1. 等待数据准备（拷贝到内核缓冲区）
2. 将数据从内核拷贝到进程中

linux系统产生了下面五种网络模式的方案：

1. 阻塞IO
2. 非阻塞IO
3. IO多路复用
4. 信号驱动IO
5. 异步IO

### 集中IO

#### 阻塞IO

![img](https://gitee.com/wanghengg/picture/raw/master/2021/1111824-20190428155107553-292331479.png)

当用户程序调用了`recvfrom`这个系统调用，`kernel`就开始了IO的第一个阶段：准备数据(对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的`UDP`包。这个时候`kernel`就要等待足够的数据到来)。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程就会被阻塞（当然，是进程自己选择的阻塞）。当`kernel`一直等到数据准备好了，它就会将数据从`kernel`中拷贝到用户内存，然后`kernel`返回结果，用户进程才解除阻塞的状态，重新运行起来。

#### 非阻塞IO

`linux`下，可以通过设置`socket`使其变为`non-blocking`。当对一个`no-blocking socket`执行读操作时：

![img](https://gitee.com/wanghengg/picture/raw/master/2021/1111824-20190428161103264-769243699.png)

当用户进程发出`read`操作时，如果`kernel`中的数据还没有准备好，那么它并不会`block`用户进程，而是立刻返回一个`error`。从用户进程角度讲，它发起的一个`read`操作后，并不需要等待，而是立马就得到了一个结果。用户进程判断结果是一个`error`时，它就知道数据还没有准备好，于是它可以再次发送`read`操作。一旦`kernel`中的数据准备好了，并且又再次收到了用户进程的`system call`，那么它立马就将数据拷贝到了用户内存，然后返回。

```c
// read函数
ssize_t read(int fd, void* buf, size_t count);
// read()会把参数fd所指的文件传送count个字节到buf指针所指的内存中。
// 返回值：返回值为实际读取的字节数，如果返回0，表示到达文件尾或是无可读取的数据。若参数count为0，则read()不会用作用并返回0。
// 注意：（1）常规文件不会阻塞，不管读到多少数据都会返回；（2）从网络设备读不一定阻塞：如果网络上没有接收到数据包，调用read会阻塞，除此之外读取的数值小于count也可能不阻塞，原因见上面链接。
```

#### IO多路复用

`IO multiplexing`就是我们说的`select，poll，epoll`，有些地方也称这种IO方式为`event driven IO`。`select/epoll`的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是`select，poll，epoll`这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。

![img](https://gitee.com/wanghengg/picture/raw/master/2021/1111824-20190428163005173-911893845.png)

```c
// select语法
int select(int nfds, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval* timeout);
/* 参数说明
* nfds：需要监视的文件描述符数量（即所有文件描述符最大值加1）
* readfds: 监视文件描述符的读变化
* writefds: 监视文件描述符的写变化
* exceptfds: 监视文件描述符的异常
* timeout: 阻塞定时
*/

// poll的功能和select类似，阻塞等待一个或者一系列文件描述符准备好IO
int poll(struct pollfd* fds, nfds_t nfds, int timeout);
struct pollfd {
    int fd;	// file descriptor
    short events;	// requested events
    short revents;	// returned events
};
// nfds指定fds的数目

// epoll操作语法
#include <sys/epoll.h>
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```



当用户进程调用了select，那么整个进程就会被block，同时kernel会 “监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。

所以，IO多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入就绪状态，select()函数就可以返回。

这里需要使用两个`system call（select 和 recvfrom）`，而blocking IO只调用了一个`system call（recvfrom）`。但是，用select的优势在于它可以同时处理多个connection。

如果处理的连接数不是很高的话，使用`select/epoll`的web server不一定比使用`mutil-threading + blocking IO`的web server性能更好，可能延迟还更大。

`select/epoll`的优势并不是对于单个连接能处理得更好，而是在于性能更多的连接。

#### 异步IO

![img](https://gitee.com/wanghengg/picture/raw/master/2021/1111824-20190428164911665-327061779.png)

用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。

### IO多路复用之select、poll、epoll详解

[select函数详解](https://blog.csdn.net/leo115/article/details/8097143)

[poll函数详解](https://blog.csdn.net/lianghe_work/article/details/46534029)

[epoll操作详解](https://blog.csdn.net/lixungogogo/article/details/52226479)

`select，poll，epoll`都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但`select，poll，epoll`本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而**<font color=red>异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。</font>**

1. select

   select函数监视文件描述符，调用后select函数会阻塞，直到有描述符就绪，或者超时，函数返回，当select函数返回后，就可以遍历描述符，找到就绪的描述符。

   select的一个缺点在于单个进程能够监视的文件描述符的数量也存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制。但是这样也会造成效率的降低。

2. poll

   没有最大限制（但是数量过大后性能也是会下降）。和select函数一样，poll返回后，需要轮询来获取就绪的描述符。

   **select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在同一时刻可能只有很少的就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。**

3. epoll

   相对于select和poll来说，**epoll最大的好处在于它不会随着监听fd数目的增长而降低效率。**相对于select和poll来说，epoll更加灵活，**没有描述符限制**。<font color=red>epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个**事件表**中，**这样在用户空间和内核空间的copy只需一次。**</font>

# 虚拟地址怎么映射到物理地址？

# 简述 IO 多路复用

# 进程间通信

# 页面调度算法

# 磁盘调度算法

# 进程调度算法

# 进程管理

## 进程的互斥与同步

## 死锁的解决方法

# malloc的实现机制

# 栈与堆有什么区别

# 进程在地址空间中会划分为哪些区域

# 零拷贝

## 传统IO

基于传统的IO方式，底层实现是通过调用`read()`和`write()`来实现的。

通过`read()`将数据从磁盘读取到内核缓冲区，然后再从内核缓冲区复制到用户缓冲区；然后通过`write()`系统调用写到socket缓冲区，最后写入网卡设备。

```c
// read()会把参数fd所指的文件传送count个字节到buf所指的内存中。
// 返回值为实际读取到的字节数，如果为0，表示已经到达文件尾或者无可读取数据。如果发生错误返回-1
ssize_t read(int fd, void* buf, size_t count);

// write()会把参数buf所指的内存中的数据的count个字节写入到fd所指的文件中。
// 如果write()顺利执行会返回实际写入的字节数。当有错误发生时返回-1，错误代码保存在errno中
ssieze_t write(int fd, const void* buf, size_t count);
```

**整个过程中发生了4次用户态和内核态的上下文切换和4次拷贝**，具体流程如下：

1. 用户进程通过`read()`方法向操作系统发起系统调用，此时上下文从用户态切换到内核态。
2. DMA控制器把数据从磁盘中拷贝到内核读缓冲区
3. CPU把读缓冲区的数据拷贝到应用缓冲区，上下文从内核态切换到用户态，`read()`返回
4. 用户进程通过`write()`方法发起系统调用，上下文从用户态切换到内核态
5. CPU将应用缓冲区数据拷贝到socket缓冲区
6. DMA控制器将数据从socket缓冲区拷贝到网卡，上下文从内核态切换到用户态，`write()`返回

![img](https://gitee.com/wanghengg/picture/raw/master/2020/v2-9aba20b7e08d93a5c3044b40b830e5eb_720w.jpg)

<font color=red>注意：</font>将数据从磁盘拷贝到内核缓冲区或者从socket缓冲区拷贝到网卡使用的DMA拷贝，将数据从内核缓冲区拷贝到用户缓冲区或者从用户缓冲区拷贝到socket缓冲区使用的是CPU拷贝。

用户空间指的就是用户进程的运行空间，内核空间就是内核的运行空间。

如果进程运行在内核空间就是内核态，运行在用户空间就是用户态。

为了安全起见，它们之间是相互隔离的，而在用户态和内核态之间的上下文切换是比较耗时的，所以在高并发场景下会对性能产生较大的影响。

## 什么是DMA拷贝呢？

对于一个IO操作而言，都是通过CPU发出对应的指令来完成，但是相比CPU来说，IO的速度太慢，CPU有大量的时间处于阻塞等待IO的状态。因此就产生了DMA（Direct Memory Access）直接内存访问技术，本质上来说它就是主板上独立的芯片，叫做DMA控制器，通过它来进行内存和IO设备的数据传输，从而减少CPU的等待时间。

但是无论谁来拷贝，频繁的拷贝耗时也是对性能的影响。

## 零拷贝

> 零拷贝技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域，减少不必要的拷贝，这种技术通常用于网络传输文件时节省CPU周期和内存带宽。

对于零拷贝技术而言，并非完全没有数据拷贝的过程，只不过是减少了用户态和内核态的切换次数以及CPU的拷贝次数。

## 常见的零拷贝技术

### mmap+write

```c
// mmap将一个文件或者其它对象映射到内存。文件被映射到多个页上，如果文件的大小不是所有页大小之和，最后一个页不被使用的部分将
// 会清空。mmap必须以PAGE_SIZE为单位进行映射。
#include <sys/mman.h>
void* mmap(void* start, size_t length, int prot, int flags, int fd, off_t offsize);
/*
* 参数start：指向欲映射的内存起始地址，通常设为NULL，代表让操作系统自动选定地址，映射成功后会返回改地址。
* 参数length：代表将文件中多大的部分映射到内存。
* 参数prot：映射内存区域的保护方式，可以用PROT_EXEC(映射区域可执行), PROT_RAED(可读), PROT_WRITE(可写), PROT_NONE
(不能存取)
* 参数flags：影响映射区域的各种特性。在调用mmap()时必须要指定MAP_SHARED(对映射区域的写入数据会写回文件)或者MAP_PRIVATE
(映射区域的写入数据不会写回文件)
* 参数fd：要映射到内存中的文件描述符。如果使用匿名内存映射时，即flags中设置了MAP_ANONYMOUS，fd设置为-1
* 参数offsize：文件映射的偏移量，通常设置为0，代表从文件的最前方开始，offset必须是分页大小的整数倍
*/
```



mmap+write简单来说就是使用mmap替换了read+write中的read操作，减少了一次CPU拷贝。

`mmap`主要实现方式是将读缓冲区的地址和用户缓冲区的地址进行映射，内核缓冲区和应用缓冲区共享，从而减少了从读缓冲区到用户缓冲区的一次CPU拷贝。

![preview](https://gitee.com/wanghengg/picture/raw/master/2020/v2-4ce7da073ca07a4c3ab1c23293edfbba_r.jpg)

整个过程中发生了4次用户天和内核态的上下文切换和3次拷贝，具体流程如下：

1. 用户进程通过`mmap()`方法向操作系统发起系统调用，上下文从用户态切换到内核态
2. DMA控制器将数据从磁盘拷贝到读缓冲区中
3. 上下文从内核态切换为用户态，`mmap()`返回
4. 用户进程通过`write()`方法发起系统调用，上下文从用户态切换到内核态
5. CPU将读缓冲区中数据拷贝到socket缓冲区
6. DMA控制器把数据从socket缓冲区拷贝到网卡，上下文从内核态切换到用户态，`write()`返回

`mmap`的方式节省了一次CPU内存拷贝，同时由于用户进程中的内存时虚拟的，只是映射到内核的读缓冲区，所以可以节省一半的内存空间，比较适合大文件的传输。

### sendfile

相比`mmap`来说，`sendfile`同样减少了一次CPU拷贝，而且还减少了2次上下文切换

```c
#include <sys/sendfile.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
// in_fd参数是待输出内容的文件描述符，out_fd是待输入内容的文件描述符。offset指定从读入文件的哪个位置开始读，如果为空，使用读
// 入文件流的默认起始位置。count参数指定文件描述符in_fd和out_fd之间传输的字节数。
```

`sendfile`是Linux2.1内核版本后引入的一个系统调用函数，通过使用`sendfile`数据可以直接在内核空间进行传输，因此避免了用户空间和内核空间的拷贝，同时由于使用`sendfile`替代了`read+write`从而节省了一次系统调用，也就是2次上下文切换。

![img](https://gitee.com/wanghengg/picture/raw/master/2020/v2-1d3d027ab59247b2a810ef51e334432b_720w.jpg)

整个过程发生了2次上下文切换，3次拷贝，具体流程如下

1. 用户进程通过`sendfile()`方法向操作系统发起系统调用，上下文从用户态转换为内核态
2. DMA控制器把数据从磁盘拷贝到读缓冲区
3. CPU将数据从内核读缓冲区拷贝到内核socket缓冲区
4. DMA控制器将socket缓冲区的数据拷贝到网卡，上下文冲内核态切换回用户态，`sendfile()`调用返回

`sendfile()`方法IO数据对用户空间完全不可见，所以只能适用于完全不需要用户空间处理的情况，比如静态文件服务器。

# 文件描述符表、打开文件表、inode表